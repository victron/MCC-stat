{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import pandas as pd\n",
    "import io\n",
    "import warnings\n",
    "import logging\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export csv\n",
    "### parce_filename:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "file format\n",
    "SYSTEMCPUSTATISTICSCORELEVEL-2022-04-22-10-10-00.csv\n",
    "\"\"\"\n",
    "filename_tuple = namedtuple('filename', ['table', 'date', 'extension'])\n",
    "\n",
    "def parce_filename(name: str) -> namedtuple:\n",
    "    fname, extension = name.split(\".\")\n",
    "    if extension != \"csv\":\n",
    "        return filename_tuple(\"\", datetime.now(), extension)\n",
    "    name_parts = fname.split(\"-\")\n",
    "    table = name_parts[0]\n",
    "    date = datetime.strptime(\"-\".join(name_parts[1:]), \"%Y-%m-%d-%H-%M-%S\")\n",
    "    return filename_tuple(table,date,extension)   \n",
    "\n",
    "# parce_filename(\"SYSTEMCPUSTATISTICSCORELEVEL-2022-04-22-10-10-00.csv\")\n",
    "# parce_filename(\"SYSTEMCPUSTATISTICSCORELEVEL-2022-04-22-10-10-00.csvq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selectFiles: selecting files based on dir location and time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectFiles(dir: str, dateStart: datetime, dateStop: datetime) -> list:\n",
    "    \"\"\"\n",
    "    selecting files based on dir location and time period\n",
    "    \"\"\"\n",
    "    fileNames = []\n",
    "    for filename in os.listdir(dir):\n",
    "        if os.path.isdir(os.path.join(dir, filename)):\n",
    "            continue\n",
    "\n",
    "        filename_info = parce_filename(filename)\n",
    "\n",
    "        if filename_info.extension != \"csv\":\n",
    "            print(\"not stat file\", filename)\n",
    "            continue\n",
    "        \n",
    "        if not dateStart < filename_info.date < dateStop:\n",
    "            continue\n",
    "        \n",
    "        fileNames.append(filename)\n",
    "    return fileNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def selectSortFiles(dir: str, dateStart: datetime, dateStop: datetime) -> dict:\n",
    "    \"\"\"\n",
    "    selecting files based on dir location and time period\n",
    "    and puting them into dict based on table name\n",
    "    \"\"\"\n",
    "    timeStart = datetime.now()\n",
    "    fileNames = {}\n",
    "    for filename in os.listdir(dir):\n",
    "        if os.path.isdir(os.path.join(dir, filename)):\n",
    "            continue\n",
    "\n",
    "        filename_info = parce_filename(filename)\n",
    "\n",
    "        if filename_info.extension != \"csv\":\n",
    "            print(\"not stat file\", filename)\n",
    "            continue\n",
    "        \n",
    "        if not dateStart < filename_info.date < dateStop:\n",
    "            continue\n",
    "        \n",
    "        tabledata = fileNames.get(filename_info.table, [])\n",
    "        if len(tabledata) == 0:\n",
    "            fileNames[filename_info.table] = tabledata\n",
    "        tabledata.append(filename)\n",
    "    print(\"selectSortFiles time=\", datetime.now()-timeStart)\n",
    "    return fileNames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wraper on panda for alarm parsing\n",
    "def pd_read_csv(filename, dtype='unicode'):\n",
    "    dateparse = lambda x: pd.to_datetime(x, format=\"%m/%d/%Y %H:%M:%S:%f\")\n",
    "    return pd.read_csv(filename, error_bad_lines=False,\n",
    "                       sep=',', header=0, index_col=6,\n",
    "                       parse_dates=['Event Time'],\n",
    "                       date_parser=dateparse, \n",
    "                       dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wraper on panda\n",
    "def pd_ems(filename, dtype=None):\n",
    "    dateparse_ems = lambda x: pd.to_datetime(x, unit='ms').tz_localize('UTC').tz_convert('Europe/Kiev')\n",
    "    return pd.read_csv(filename, error_bad_lines=False,\n",
    "                       sep=',', header=0, index_col=0,\n",
    "                       parse_dates=['#timeofcollection'],\n",
    "                       date_parser=dateparse_ems, \n",
    "#                        for autoconverting types\n",
    "                       dtype=dtype, na_values=''\n",
    "                      )\n",
    "\n",
    "def read_ems_csv(filename, dtype=None, tz: str = \"UTC\"):\n",
    "    \"\"\"\n",
    "    read csv file from EMS to df\n",
    "    \"\"\"\n",
    "    dateparse_ems = lambda x: pd.to_datetime(x, unit='ms').tz_localize('UTC').tz_convert(tz)\n",
    "    return pd.read_csv(filename, on_bad_lines=\"skip\",\n",
    "                       sep=',', header=0, index_col=0,\n",
    "                       parse_dates=['#timeofcollection'],\n",
    "                       date_parser=dateparse_ems, \n",
    "#                        for autoconverting types\n",
    "                       dtype=dtype, na_values=''\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what fields to mark as category. done manualy\n",
    "# TODO: generate this dict from file mccPerfObjects.csv or like PORT_STATS.xml\n",
    "cat_all = set(['name',\n",
    " 'operatingSystems_grid',\n",
    " 'resourceid',\n",
    " 'zone_name',\n",
    " 'protocols_grid',\n",
    " 'category',\n",
    " 'applications_grid',\n",
    " 'diameterInterfaceType_intfType',\n",
    " 'diameterInterfaceType_nodeName',\n",
    " 'diameterInterfaceType_peerName',\n",
    " 'serviceDataContainerUsageStatsNew_ratingGroupName',\n",
    " 'serviceDataContainerUsageStatsNew_serviceIdentifier',\n",
    " 'epdgApn_name',\n",
    " 'apn_name',\n",
    " 'wagApn_name',\n",
    " 'sgwApn_name',\n",
    " 'gtpMessage_gwName',\n",
    " 'gtpMessage_messageType',\n",
    " 'gw_name',\n",
    " 'qci',\n",
    " 'arp',\n",
    " 'gw_type',\n",
    " 'peer_chassis',\n",
    " 'peer_name',\n",
    " 'buffer_chassis',\n",
    " 'buffer_slot',\n",
    " 'bearerUsageStatsNew_gatewayType',\n",
    " 'bearerUsageStatsNew_gatewayName',\n",
    " 'bearerUsageStatsNew_qci',\n",
    " 'networkContext_name',\n",
    " 'uePoolStatistics_name',\n",
    " 'networkContextStatistics_name',\n",
    " 'ueIpSubPoolStatistics_name',\n",
    " 'interfaceStatistics_name',\n",
    " 'interfaceUsageStatsNew_gatewayType',\n",
    " 'interfaceUsageStatsNew_gatewayName',\n",
    " 'interfaceUsageStatsNew_interfaceType',\n",
    " 'interfaceUsageStatsNew_interfaceName',\n",
    " 'chassis_id',\n",
    " 'card_id',\n",
    " 'portstats_name',\n",
    " 'summary_chassis',\n",
    " 'summary_slot',\n",
    " 'connection_chassis',\n",
    " 'connection_slot',\n",
    " 'module-stats_chassis',\n",
    " 'module-stats_slot',\n",
    " 'systemCpuStats_chassis',\n",
    " 'systemCpuStats_slot',\n",
    " 'systemCpuStats_cpu',\n",
    " 'systemCpuStats_core',\n",
    " 'systemMemoryStats_chassis',\n",
    " 'systemMemoryStats_slot',\n",
    " 'systemMemoryStats_cpu',\n",
    " 'diskPerfStatus_diskname',\n",
    " 'resultCodeStatistics_intfType',\n",
    " 'resultCodeStatistics_nodeName',\n",
    " 'resultCodeStatistics_peerName',\n",
    " 'resultCodeStatistics_resultCode',\n",
    " 'pgw_name',\n",
    " 'stats_slot',\n",
    " 'stats_cpuNum',\n",
    "'segment_chassis',\n",
    " 'session_chassis',\n",
    " 'detail_chassis',\n",
    " 'detail_chassis',\n",
    " 'contentCategorization_chassis',\n",
    " 'contentCategorization_slot',\n",
    " 'contentCategorization_chassis',\n",
    " 'contentCategorization_slot',\n",
    " 'detail_chassis',\n",
    " 'detail_chassis',\n",
    " 'failure_chassis',\n",
    " 'latencyStats_chassis',\n",
    " 'overloadControl_chassis',\n",
    " 'tcpOptz_chassis',\n",
    " 'tcpOptz_slot',\n",
    " 'bytes_chassis',\n",
    " 'optzsavings_chassis',\n",
    " 'perf-stats_chassis',\n",
    " 'perf-stats_slot',\n",
    " 'perf-stats_chassis',\n",
    " 'natModuleStats_chassis',\n",
    " 'all_chassis',\n",
    " 'all_chassis',\n",
    " 'pluginEntry_chassis',\n",
    " 'pluginEntry_chassis',\n",
    " 'advanced_chassis',\n",
    " 'basic_chassis',\n",
    " 'systemLoadStats_chassis',\n",
    " 'systemLoadStats_slot',\n",
    " 'systemPfmStats_chassis',\n",
    " 'systemPfmStats_slot',\n",
    " 'server-side_chassis',\n",
    " 'tac_chassis',\n",
    " 'tcpReOrderPerfStats_chassis',\n",
    " 'tcpReOrderPerfStats_chassis',\n",
    " 'tcpReOrderPerfStats_slot',\n",
    " 'rateLimiting_chassis',\n",
    " 'detail_chassis',\n",
    " 'dns_chassis',\n",
    " 'failure_chassis',\n",
    " 'detail_chassis',\n",
    " 'dns_chassis',\n",
    " 'failure_chassis',\n",
    " 'quic_chassis',\n",
    " 'quic_chassis',\n",
    " 'rateLimiting_chassis',\n",
    " 'optzsavings_chassis',\n",
    " 'perfStats_chassis',\n",
    " 'perfStats_chassis',\n",
    " 'perfStats_slot',\n",
    "'compression_grid',\n",
    " 'compression_grid',\n",
    " 'premptDns_grid',\n",
    " 'detail_grid',\n",
    " 'summary_grid',\n",
    " 'summary_grid',\n",
    " 'summary_grid',\n",
    " 'detail_grid',\n",
    " 'wapgw_grid',\n",
    " 'general_grid',\n",
    " 'general_grid',\n",
    " 'tethering_grid',\n",
    " 'HttpTransactions_grid',\n",
    "               'uptime_id',\n",
    " 'uptime_nodeNumber',\n",
    " 'uptime_cpuNumber',\n",
    "       'statistics_cpuNumber',\n",
    "             'dns_name',\n",
    "'edrBearerAggrStats_networkContext', 'edrBearerAggrStats_name',\n",
    "'edrFlowAggrStats_networkContext', 'edrFlowAggrStats_name',\n",
    "'edrHttpAggrStats_networkContext', 'edrHttpAggrStats_name',\n",
    "'edrPilotAggrStats_networkContext', 'edrPilotAggrStats_name',\n",
    "'edrRTTAggrStats_networkContext', 'edrRTTAggrStats_name',\n",
    "'edrServerAggrStats2_networkContext', 'edrServerAggrStats2_name',\n",
    "'edrSessionAggrStats_networkContext', 'edrSessionAggrStats_name',\n",
    "'applicationAttrs_apn', 'applicationAttrs_groupName', 'applicationAttrs_applicationName', 'applicationAttrs_attributeName',\n",
    "'flow-attrs_apn', 'flow-attrs_name',\n",
    "'protocolAttrs_apn', 'protocolAttrs_groupName', 'protocolAttrs_protocolName', 'protocolAttrs_attributeName',\n",
    "'tdci-stats_apn', 'tdci-stats_method',\n",
    "               'geoGroup_name',\n",
    "'pcscfAddressInUse_apnName', 'pcscfAddressInUse_addressName',\n",
    " 'saegwPgw_name', 'saegwPgw_name','pluginEntry_name','pluginEntry_name','serviceDataContainerUsageStatsNew_gatewayType', \n",
    "               'serviceDataContainerUsageStatsNew_gatewayName' ,\n",
    "               'statistics_taskname', 'uplaneDns_name'\n",
    "               \n",
    "              ])\n",
    "\n",
    "# dtypes generator\n",
    "categories_all = {c: 'category' for c in cat_all}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv(filenames: list):\n",
    "    \"\"\"\n",
    "    read list of files and create on stringIO file.\n",
    "    Headers from files removed exept first file\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    vfile = io.StringIO(newline='\\n')\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"r\") as file:\n",
    "            if count == 0:\n",
    "                vfile.write(file.read())\n",
    "                count+=1\n",
    "                continue\n",
    "            vfile.write(\"\\n\".join(file.readlines()[1:]))\n",
    "        count+=1\n",
    "    # print(\"vfile=\", vfile.getvalue())\n",
    "    vfile.seek(0) # move cursor to possition 0. begin of file\n",
    "    return vfile\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_frames_old(stat_dir: str, period_start: str, period_stop: str, tz: str = \"UTC\") -> dict:\n",
    "    \"\"\"\n",
    "    creates dictionary of stat frames\n",
    "    stat_dir: directory with stat files\n",
    "    period_start: start time in format 2022-04-22T10:00\n",
    "    period_stop: stop time\n",
    "    \n",
    "    \"\"\"\n",
    "    parse_period_start = datetime.strptime(period_start, \"%Y-%m-%dT%H:%M\")\n",
    "    parse_period_stop = datetime.strptime(period_stop, \"%Y-%m-%dT%H:%M\")\n",
    "    selected_files = selectSortFiles(stat_dir, parse_period_start, parse_period_stop)\n",
    "    \n",
    "    def read2pd_files(fileNames: list):\n",
    "        def custom_readcsv(args):\n",
    "            dateparse_ems = lambda x: pd.to_datetime(x, unit='ms').tz_localize('UTC').tz_convert(tz)\n",
    "            return pd.read_csv(args, on_bad_lines=\"skip\",\n",
    "                               sep=',', header=0, index_col=0,\n",
    "                               parse_dates=['#timeofcollection'],\n",
    "                               date_parser=dateparse_ems, \n",
    "        #                        for autoconverting types\n",
    "                               dtype=categories_all, na_values=''\n",
    "                              )\n",
    "        return pd.concat(map(custom_readcsv, fileNames))\n",
    "    \n",
    "    stat = {}\n",
    "    timeStart = datetime.now()\n",
    "    for k, v in selected_files.items():\n",
    "        stat[k] = read2pd_files([os.path.join(DATAs, f) for f in v])\n",
    "    print(\"concat time=\", datetime.now()-timeStart)\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove_unnamed: removing columns like `Unnamed: 10` from all df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_unnamed(stat: dict):\n",
    "    \"\"\"\n",
    "    removing columns like `Unnamed: 10` from all df\n",
    "    \"\"\"\n",
    "    for key in stat.keys():\n",
    "        for columnName in stat[key].columns:\n",
    "            if columnName.find('Unnamed:') > -1:\n",
    "                stat[key].drop(columns=columnName, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stat_frames: main func to parse CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_frames(stat_dir: str, period_start: str, period_stop: str, tz: str = \"UTC\") -> dict:\n",
    "    \"\"\"\n",
    "    using merge_csv function to merge files in one virtual before puting into df. This working\n",
    "    quicker then df concat\n",
    "    \n",
    "    creates dictionary of stat frames\n",
    "    stat_dir: directory with stat files\n",
    "    period_start: start time in format 2022-04-22T10:00\n",
    "    period_stop: stop time\n",
    "    \n",
    "    \"\"\"\n",
    "    parse_period_start = datetime.strptime(period_start, \"%Y-%m-%dT%H:%M\")\n",
    "    parse_period_stop = datetime.strptime(period_stop, \"%Y-%m-%dT%H:%M\")\n",
    "    selected_files = selectSortFiles(stat_dir, parse_period_start, parse_period_stop)\n",
    "    \n",
    "    stat = {}\n",
    "    timeStart = datetime.now()\n",
    "    num_tabels = len(selected_files)\n",
    "    count = 0\n",
    "    for table, filenames in selected_files.items():\n",
    "        # print(\"working on table=\", table)\n",
    "        vfile = merge_csv([os.path.join(stat_dir, f) for f in filenames])\n",
    "        stat[table] = read_ems_csv(vfile, dtype=categories_all, tz=tz)\n",
    "        vfile.close()\n",
    "        count +=1\n",
    "        print(f'done: {count}/{num_tabels}', end='\\r', flush=True) # progress bar                      \n",
    "    print(\"put in dfs time=\", datetime.now()-timeStart)\n",
    "    \n",
    "    remove_unnamed(stat)\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table manipulation\n",
    "### find_cat_counters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cat_counters(table: dict, unique=50):\n",
    "    possible_col = []\n",
    "    possible_col_less = []\n",
    "    for name, df in table.items():\n",
    "        obj_columns = [col for col in df.columns if pd.api.types.is_object_dtype(df[col])]\n",
    "        int_columns = [col for col in df.columns if pd.api.types.is_integer_dtype(df[col])]\n",
    "        print(f'----- table= {name} -------')\n",
    "        if len(obj_columns) > 0:\n",
    "            print(f'contains objects: {obj_columns}')\n",
    "        if len(int_columns) > 0:\n",
    "            for col in int_columns:\n",
    "                if (len(df[col].unique()) < unique) and (df[col].sum()>0) :\n",
    "                    print(f'possible cat= {col}')\n",
    "                    test_word = col.lower()\n",
    "                    if (test_word.find('chassis') > -1) or (test_word.find('slot') > -1) or (test_word.find('_grid') > -1) or (test_word.find('_nodenumber') > -1) or (test_word.find('_cpunumber') > -1):\n",
    "                        possible_col.append(col)\n",
    "                    if test_word.startswith('num') or (test_word.find('_min') > -1) or (test_word.find('_max') > -1) or (test_word.find('_avg') > -1):\n",
    "                        continue\n",
    "                    else:\n",
    "                        possible_col_less.append(col)\n",
    "    return (possible_col, possible_col_less)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF manipulation\n",
    "### scrab: removing from view some columns:\n",
    "- columns where there is `0` in data (as sum)\n",
    "- columns with unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrab(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    _df = df.copy()\n",
    "    count_before = len(df.columns)\n",
    "    cat_columns = [col for col in _df.columns if type(_df[col].dtypes) is pd.CategoricalDtype]\n",
    "    cat_unique = [col for col in cat_columns if len(_df[col].unique()) < 2]\n",
    "    _df.drop(columns=cat_unique, inplace=True)\n",
    "#     TODO: get objects columns\n",
    "    \n",
    "    # if _df.index.is_unique != True:\n",
    "    #     print('WARNING:', 'index in dataframe not unique')\n",
    "    # new_df = df.loc[:, [(df[col]>0).any() for col in df.columns ]   \n",
    "#     in if statemet used lazy condition\n",
    "    filter =[col for col in _df.columns if col in cat_columns or (_df[col].sum()>0)]\n",
    "    new_df = _df.loc[:, filter ]\n",
    "    logging.debug(f\"scrab result columns before: {count_before}, after: {len(new_df.columns)}\")\n",
    "    # print(f\"scrab result columns before: {count_before}, after: {len(new_df.columns)}\")\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find_columns: return list of columns with a search sub word in column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_columns(df: pd.DataFrame, word: str) -> list:\n",
    "    return [col for col in df.columns if col.lower().find(word) > -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### info: mostly interesting params of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = namedtuple('df_info', ['unique', 'monotonic', 'start', 'stop', 'len'])\n",
    "def info(df: pd.DataFrame) -> namedtuple:\n",
    "    start, stop = df.index[0], df.index[-1]\n",
    "    all_dates = pd.date_range(start=start, end=stop, freq='5min', tz='UTC')\n",
    "    unique, monotonic = df.index.is_unique, df.index.is_monotonic_increasing\n",
    "    lenn = df.shape[0]\n",
    "\n",
    "    # logging.info(f'is_unique= {unique} is_monotonic= {monotonic} len= {lenn}')\n",
    "    # logging.info(f'start= {start} stop= {stop}')\n",
    "    if unique and monotonic:\n",
    "        logging.warning(f'missing dates= {all_dates.difference(df.index)}')\n",
    "    return df_info(unique, monotonic, start, stop, lenn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top_corr: print top corr to some column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_corr(df: pd.DataFrame, column: str, top_num=20):\n",
    "    _df = dataScrab(df)             # removing columns with 0, to have smaller df\n",
    "    corr = _df.corr()\n",
    "# unstack creates multiindex, so just select on first one\n",
    "    print(f'----- shape= {df.shape} -----')\n",
    "    print(corr.unstack()[column].sort_values(ascending=False).head(top_num))\n",
    "    print('----- top negative ------')\n",
    "    print(corr.unstack()[column].sort_values(ascending=True).head(top_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top_corrwith: print top corr to some column\n",
    "correlate df with series and applying optionaly lag  \n",
    "`+` lag means shift forward series - first lines is NaN  \n",
    "`-` lag means shift backward series - last lines is NaN  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_corrwith(df: pd.DataFrame, series: pd.Series, lag=0, top_num=20):\n",
    "    _df = dataScrab(df)\n",
    "    print(f'----- shape= {df.shape} -----')\n",
    "    if lag != 0:\n",
    "        print('------ lag=\"+\" means first lines in serias is NaN -----')\n",
    "    print(_df.corrwith(series.shift(lag)).sort_values(ascending=False).head(top_num))\n",
    "    print('----- top negative ------')\n",
    "    print(_df.corrwith(series.shift(lag)).sort_values(ascending=True).head(top_num))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pivot: usind pivod method creating table with unique and monotonic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    _df_t =  scrab(df.copy())\n",
    "    cat_columns = [col for col in _df_t.columns if pd.api.types.is_categorical_dtype(_df_t[col])]\n",
    "    obj_columns = [col for col in _df_t.columns if pd.api.types.is_object_dtype(_df_t[col])]\n",
    "    assert len(obj_columns) == 0, f'there is obj_columns= {obj_columns}'\n",
    "            \n",
    "    _df_new = _df_t .pivot(columns=cat_columns)\n",
    "    assert _df_new.index.is_unique == True, 'index in new df not unique'\n",
    "    if not _df_new.index.is_monotonic:\n",
    "        warnings.warn('df not monotonic')\n",
    "        \n",
    "    return scrab(_df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corrwith: \n",
    "Notes:\n",
    "function dependency ==> correlation\n",
    "but\n",
    "correlation !=> function dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrwith(df: pd.DataFrame, series: pd.Series, diff=1, lag=0, \n",
    "                 method='pearson') -> tuple:\n",
    "    if not df.index.is_unique:\n",
    "        _df = pivot(df)\n",
    "    else:\n",
    "        _df = scrab(df.copy())\n",
    "    if not _df.index.is_monotonic:\n",
    "        warnings.warn('df not monotonic')\n",
    "        \n",
    "    assert series.index.is_unique, 'reference series NOT unique'\n",
    "    _sr = series.copy()\n",
    "    \n",
    "    if info(_df) != info(_sr):\n",
    "        logging.warning('-- main params not same for df and sr --')\n",
    "        info(_df)\n",
    "        logging.warning('----- series -----')\n",
    "        info(_sr)\n",
    "    \n",
    "    # if NaN present could happening situation when there is small number of points is present\n",
    "    # as result correlation is high on this small number\n",
    "# TODO:\n",
    "# - fillna with random noise\n",
    "# - remove from _df columns with na values bigger then in _sr\n",
    "    _df0 = _df.fillna(0)\n",
    "    _df0 = _df0.diff(diff)\n",
    "    _sr = _sr.diff(diff)\n",
    "\n",
    "    corr = _df0.corrwith(_sr.shift(lag), drop=True, method=method).dropna()\n",
    "        \n",
    "    return (corr, _df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### corrwith_old: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrwith_old(df: pd.DataFrame, series: pd.Series, diff=1, lag=0, \n",
    "                  printt=True, method='pearson') -> tuple:\n",
    "    if not df.index.is_unique:\n",
    "        _df = pivot(df)\n",
    "    else:\n",
    "        _df = scrab(df.copy())\n",
    "    if not _df.index.is_monotonic:\n",
    "        warnings.warn('df not monotonic')\n",
    "        \n",
    "    assert series.index.is_unique, 'reference series NOT unique'\n",
    "    _sr = series.copy()\n",
    "    \n",
    "    if info(_df, printt=False) != info(_sr, printt=False):\n",
    "        warnings.warn('main params not same for df and sr')\n",
    "        info(_df)\n",
    "        print('----- series -----')\n",
    "        info(_sr)\n",
    "    \n",
    "    # if NaN present could happening situation when there is small number of points is present\n",
    "    # as result correlation is high on this small number\n",
    "# TODO:\n",
    "# - fillna with random noise\n",
    "# - remove from _df columns with na values bigger then in _sr\n",
    "    _df0 = _df.fillna(0)\n",
    "    _df0 = _df0.diff(diff)\n",
    "    _sr = _sr.diff(diff)\n",
    "\n",
    "    corr = _df0.corrwith(_sr.shift(lag), drop=True, method=method).dropna()\n",
    "    # corr = _df0.corrwith(_sr.shift(lag), drop=False, method=method).dropna()\n",
    "    # corr_top_p = corr.sort_values(ascending=False).head(top_num)\n",
    "    # corr_top_n = corr.sort_values(ascending=True).head(top_num)\n",
    "    # if printt:\n",
    "    #     print(f'----- shape _df0= {_df0.shape}; _sr= {_sr.shape} -----')\n",
    "    #     if lag != 0:\n",
    "    #         print('------ lag=\"+\" means first lines in serias is NaN -----')\n",
    "    #     print(corr_top_p)\n",
    "    #     print('----- top negative ------')\n",
    "    #     print(corr_top_n)\n",
    "        \n",
    "    return (corr, _df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot_corr: helper create mutiple plots for corr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr2(x: str, stat_unique: dict = None,\n",
    "               dates: pd.IndexSlice = pd.IndexSlice[:], axvlines=[]):\n",
    "    _df = stat_unique[x['table']]\n",
    "    ax = _df.loc[dates, x['index']].plot(\n",
    "        legend=True, ax=x['ax'], ylabel=x['ylabel'])\n",
    "    for line in axvlines:\n",
    "        ax.axvline(line, color=\"red\", linestyle=\"--\")\n",
    "    print(f'{x[\"ylabel\"]} {x[\"table\"]} {x[\"index\"]}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(corr_all: pd.DataFrame, corr_filter: pd.Series,\n",
    "              testdf: pd.DataFrame, stat_unique: dict,\n",
    "              dates: pd.IndexSlice = pd.IndexSlice[:], axvlines=[]):\n",
    "    width, height = 15, 3\n",
    "\n",
    "    c = corr_all[corr_filter].copy()\n",
    "    assert c.shape[0] > 0, 'no correlations with requested corr_filter, check filter'\n",
    "    c.sort_values('corr', key=np.abs, inplace=True)\n",
    "\n",
    "    rows = c.shape[0]\n",
    "    fig, axes = plt.subplots(rows+1, 1, figsize=(width, rows*height))\n",
    "    # plotting ref series\n",
    "    ax = testdf[dates].plot(legend=True, ax=axes[0], ylabel='ref')\n",
    "    for line in axvlines:\n",
    "        ax.axvline(line, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    # ploting correlated series\n",
    "    c['ax'], c.loc[:, 'ylabel'] = axes[1:], range(1, rows+1) # adding params to df for plotting\n",
    "    c.apply(plot_corr2, stat_unique=stat_unique, dates=dates,\n",
    "            axvlines=axvlines, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_old(corr_all: pd.DataFrame, corr_filter: pd.Series, \n",
    "              testdf: pd.DataFrame, stat_unique: dict,\n",
    "              dates: pd.IndexSlice =pd.IndexSlice[:], axvlines=[]):\n",
    "    width, height = 15, 3 \n",
    "    \n",
    "    c = corr_all.copy()\n",
    "    c = corr_all[corr_filter]\n",
    "    assert c.shape[0] > 0, 'no correlations with requested corr_filter, check filter'\n",
    "    c.sort_values('corr', key=np.abs, inplace=True)\n",
    "    \n",
    "    rows = c.shape[0]\n",
    "    fig, axes = plt.subplots(rows+1, 1, figsize=(width, rows*height))\n",
    "    testdf[dates].plot(legend=True, ax=axes[0], ylabel='0')\n",
    "    i = 1\n",
    "    for _, row in c.iterrows():\n",
    "        df = stat_unique[row['table']]\n",
    "        ax = df.loc[dates, row['index']].plot(legend=True, ax=axes[i], ylabel=i)\n",
    "        for line in axvlines:\n",
    "            ax.axvline(line, color=\"red\", linestyle=\"--\")\n",
    "        print(f'{i} {row[\"index\"]} {row[\"table\"]}')\n",
    "        i=i+1\n",
    "#     print c df for easy selections\n",
    "    # c.index = range(1,rows+1)\n",
    "    # display(c)\n",
    "# correct method, but I don't know how to change arg for diff rows\n",
    "# c.apply(lambda x: plot_corr(stat_unique, x), axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corrAll:\n",
    "creating corr matrix for all tables\n",
    "stat: dict - with all tables, before pivot  \n",
    "sr: pd.Series - test series, to correlate with  \n",
    "-> tuple - (pd.DataFrame, dict) - df in format `index \tcorr \ttable`, dict contains  stat_unique table (after pivot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrAll(stat: dict, sr: pd.Series) -> tuple:\n",
    "    stat_unique = {}\n",
    "    corr_all = pd.DataFrame(columns=['index', 'corr', 'table'])\n",
    "    num_tabels = len(stat)\n",
    "    count = 0\n",
    "    for k, v in stat.items():\n",
    "        try:\n",
    "            corr, stat_unique[k] = corrwith(v, sr)\n",
    "        except Exception as e:\n",
    "            logging.error(f'error in k= {k}')\n",
    "            logging.error(f'{e}')\n",
    "            break\n",
    "\n",
    "        dfcorr = pd.DataFrame({'index': corr.index.values, 'corr':corr.values})\n",
    "        dfcorr['table'] = k\n",
    "        corr_all = pd.concat([corr_all, dfcorr], ignore_index=True)\n",
    "        count +=1\n",
    "        print(f'done: {count}/{num_tabels}, finished {k}', end='\\r', flush=True)\n",
    "    print(corr_all.shape)\n",
    "    return corr_all, stat_unique\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
